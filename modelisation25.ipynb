{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš— Preuve de Concept : Segmentation SÃ©mantique pour VÃ©hicules Autonomes\n",
    "\n",
    "Ce notebook implÃ©mente un pipeline de segmentation sÃ©mantique comparant une architecture classique (**U-Net**) et des modÃ¨les d'Ã©tat de l'art (**YOLO**).\n",
    "\n",
    "## ðŸ“‘ Plan du Notebook\n",
    "1. **Installation et Importations** : PrÃ©paration de l'environnement de travail.\n",
    "2. **Configuration et DonnÃ©es** : Mapping des classes Cityscapes et configuration globale.\n",
    "3. **Augmentation de DonnÃ©es** : ImplÃ©mentation de la technique **CutMix**.\n",
    "4. **Dataset et MÃ©triques** : PrÃ©paration du chargeur de donnÃ©es et des fonctions d'Ã©valuation.\n",
    "5. **Analyse Exploratoire (EDA)** : Visualisation de la distribution et exemples.\n",
    "6. **EntraÃ®nement** : Fonctions d'entraÃ®nement pour U-Net et YOLO.\n",
    "7. **Visualisation** : Outils de gÃ©nÃ©ration de masques colorÃ©s.\n",
    "8. **ExÃ©cution et Comparaison** : Lancement des entraÃ®nements et affichage des rÃ©sultats.\n",
    "9. **Optimisation et Analyse** : Optimisation du meilleur modÃ¨le et analyse approfondie.\n",
    "10. **Conclusion** : SynthÃ¨se des rÃ©sultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des librairies nÃ©cessaires\n",
    "!pip install torch torchvision opencv-python pandas ultralytics segmentation-models-pytorch albumentations\n",
    "\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import yaml\n",
    "import glob\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import segmentation_models_pytorch as smp\n",
    "from ultralytics import YOLO\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, jaccard_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration et PrÃ©paration des DonnÃ©es\n",
    "Nous dÃ©finissons ici le mapping des classes pour regrouper les 34 classes originales de Cityscapes en 8 catÃ©gories principales (VÃ©hicules, Humains, Route, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mapping des classes Cityscapes (8 Classes) ---\n",
    "# Inclut la classe 0 (Route/Flat).\n",
    "\n",
    "cityscapes_mapping = {\n",
    "    0: 7, 1: 7, 2: 7, 3: 7, 4: 7, 5: 7, 6: 7, # void\n",
    "    7: 0, 8: 0, 9: 0, 10: 0, # road\n",
    "    11: 3, 12: 3, 13: 3, 14: 3, 15: 3, 16: 3, # construction\n",
    "    17: 4, 18: 4, 19: 4, 20: 4, # object\n",
    "    21: 5, 22: 5, # nature\n",
    "    23: 6, # sky\n",
    "    24: 1, 25: 1, # human\n",
    "    26: 2, 27: 2, 28: 2, 29: 2, 30: 2, 31: 2, 32: 2, 33: 2 # vehicle\n",
    "}\n",
    "\n",
    "# Groupes originaux : 0=Road, 1=Human, 2=Vehicle, 3=Const., 4=Object, 5=Nature, 6=Sky, 7=Void\n",
    "# Nouveau mapping : 0..7 -> 0..7\n",
    "id_to_group = np.full(256, 255, dtype=np.uint8) # Default to ignore for unmapped Cityscapes IDs\n",
    "for id_val, group_id in cityscapes_mapping.items():\n",
    "    id_to_group[id_val] = group_id # Direct mapping to new 0-7 classes\n",
    "\n",
    "# Couleurs pour la visualisation (RGB)\n",
    "original_colors = [[128, 64, 128], [220, 20, 60], [0, 0, 142], [70, 70, 70], [220, 220, 0], [107, 142, 35], [70, 130, 180], [0, 0, 0]]\n",
    "group_colors = original_colors # Use all original colors\n",
    "NUM_CLASSES = 8 # Now 8 classes\n",
    "\n",
    "# --- Configuration Globale ---\n",
    "BASE_PATH = '/content/drive/MyDrive/P10_Developpez une preuve de concept'\n",
    "DATA_DIR = os.path.join(BASE_PATH, 'data')\n",
    "\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'img_size': (256, 256),\n",
    "    'test_images_dir': os.path.join(DATA_DIR, 'images_tests'),\n",
    "    'output_dir': os.path.join(BASE_PATH, 'results'),\n",
    "    'yolo_runs_dir': os.path.join(BASE_PATH, 'yolo_runs'),\n",
    "    'mini_unet': {\n",
    "        'img_dir': os.path.join(DATA_DIR, 'img'),\n",
    "        'lbl_dir': os.path.join(DATA_DIR, 'label'),\n",
    "        'lr': 0.0001,\n",
    "        'batch_size': 8,\n",
    "        'epochs': 20,\n",
    "        'backbone': 'vgg16',\n",
    "        'use_cutmix': True\n",
    "    },\n",
    "    'yolo_models': [\n",
    "        {'name': 'YOLOv8n', 'version': 'yolov8n-seg.pt', 'data': os.path.join(DATA_DIR, 'yoloautodrivesegmentation.v5-v5.yolov8/data.yaml')},\n",
    "        {'name': 'YOLOv8s', 'version': 'yolov8s-seg.pt', 'data': os.path.join(DATA_DIR, 'yoloautodrivesegmentation.v5-v5.yolov8/data.yaml')},\n",
    "        {'name': 'YOLOv8m', 'version': 'yolov8m-seg.pt', 'data': os.path.join(DATA_DIR, 'yoloautodrivesegmentation.v5-v5.yolov8/data.yaml')},\n",
    "        {'name': 'YOLOv9', 'version': 'yolov9c-seg.pt', 'data': os.path.join(DATA_DIR, 'yoloautodrivesegmentation.v5-v5.yolov9/data.yaml')},\n",
    "        {'name': 'YOLOv9e', 'version': 'yolov9e-seg.pt', 'data': os.path.join(DATA_DIR, 'yoloautodrivesegmentation.v5-v5.yolov9/data.yaml')},\n",
    "        {'name': 'YOLOv11n', 'version': 'yolo11n-seg.pt', 'data': os.path.join(DATA_DIR, 'yoloautodrivesegmentation.v5-v5.yolov11/data.yaml')},\n",
    "    ]\n",
    "}\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Fixe la graine alÃ©atoire pour la reproductibilitÃ©.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "# --- Correction Dynamique du YAML (Fix Path Error) ---\n",
    "def fix_yaml_paths(yaml_path):\n",
    "    \"\"\"Corrige les chemins dans le fichier data.yaml pour pointer vers DATA_DIR.\"\"\"\n",
    "    if not os.path.exists(yaml_path):\n",
    "        return\n",
    "\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "\n",
    "    # On dÃ©finit le dossier parent du fichier yaml comme racine du dataset\n",
    "    dataset_root = os.path.dirname(yaml_path)\n",
    "\n",
    "    # Mise Ã  jour des clÃ©s standards de YOLO\n",
    "    data['path'] = dataset_root\n",
    "    data['train'] = 'train/images'\n",
    "    data['val'] = 'valid/images'\n",
    "    if 'test' in data:\n",
    "        data['test'] = 'test/images'\n",
    "\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(data, f, default_flow_style=False)\n",
    "    print(f\"ðŸ”§ YAML corrigÃ© : {yaml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Augmentation de DonnÃ©es : CutMix\n",
    "La technique **CutMix** consiste Ã  dÃ©couper une partie d'une image et Ã  la coller sur une autre, tout en mÃ©langeant les Ã©tiquettes (labels) proportionnellement. Cela force le modÃ¨le Ã  ne pas se fier uniquement Ã  des caractÃ©ristiques locales Ã©videntes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ImplÃ©mentation CutMix ---\n",
    "def apply_cutmix(images, masks, alpha=1.0):\n",
    "    \"\"\"Applique l'augmentation CutMix sur un batch d'images et de masques.\"\"\"\n",
    "    indices = torch.randperm(images.size(0))\n",
    "    shuffled_images, shuffled_masks = images[indices], masks[indices]\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    W, H = images.size(2), images.size(3)\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w, cut_h = np.int32(W * cut_rat), np.int32(H * cut_rat)\n",
    "    cx, cy = np.random.randint(W), np.random.randint(H)\n",
    "    bbx1, bby1 = np.clip(cx - cut_w // 2, 0, W), np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2, bby2 = np.clip(cx + cut_w // 2, 0, W), np.clip(cy + cut_h // 2, 0, H)\n",
    "    images[:, :, bbx1:bbx2, bby1:bby2] = shuffled_images[:, :, bbx1:bbx2, bby1:bby2]\n",
    "    masks[:, bbx1:bbx2, bby1:bby2] = shuffled_masks[:, bbx1:bbx2, bby1:bby2]\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset et MÃ©triques\n",
    "Nous dÃ©finissons ici la classe `SegmentationDataset` pour charger les images et les masques, ainsi que les mÃ©triques d'Ã©valuation (Accuracy et mIoU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset & Metrics ---\n",
    "def calculate_pixel_metrics(y_true, y_pred):\n",
    "    \"\"\"Calcule l'accuracy pixel par pixel et l'IoU moyen (mIoU) en ignorant l'index 255.\"\"\"\n",
    "    y_true, y_pred = y_true.flatten(), y_pred.flatten()\n",
    "\n",
    "    # Filtrer les pixels ignorÃ©s (255)\n",
    "    mask = y_true != 255\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    if len(y_true) == 0:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    iou = jaccard_score(y_true, y_pred, average='macro', zero_division=1.0)\n",
    "    return acc, iou\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        self.img_dir, self.mask_dir = img_dir, mask_dir\n",
    "        self.images = sorted([f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png'))])\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        img_p = os.path.join(self.img_dir, self.images[idx])\n",
    "        mask_p = os.path.join(self.mask_dir, self.images[idx])\n",
    "        image = cv2.cvtColor(cv2.imread(img_p), cv2.COLOR_BGR2RGB)\n",
    "        # Conversion du masque en niveaux de gris vers les groupes d'ID\n",
    "        # id_to_group gÃ¨re maintenant le remapping et l'ignore index (255)\n",
    "        mask = id_to_group[cv2.imread(mask_p, cv2.IMREAD_GRAYSCALE)]\n",
    "        if self.transform:\n",
    "            aug = self.transform(image=image, mask=mask)\n",
    "            image, mask = aug['image'], aug['mask']\n",
    "        return image, mask.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse Exploratoire des DonnÃ©es (EDA)\n",
    "Avant de lancer l'entraÃ®nement, nous visualisons la distribution des classes et quelques exemples d'images avec leurs masques pour vÃ©rifier la qualitÃ© des donnÃ©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyse Exploratoire (EDA) ---\n",
    "print(\"\\nðŸ“Š Lancement de l'Analyse Exploratoire (EDA)...\")\n",
    "\n",
    "def perform_eda():\n",
    "    img_dir, lbl_dir = CONFIG['mini_unet']['img_dir'], CONFIG['mini_unet']['lbl_dir']\n",
    "    if not os.path.exists(img_dir):\n",
    "        print(\"âš ï¸ Dossier d'images introuvable. EDA ignorÃ©e.\")\n",
    "        return\n",
    "\n",
    "    # Dataset pour EDA (Resize + ToTensor, pas de normalisation pour affichage direct)\n",
    "    eda_transform = A.Compose([A.Resize(*CONFIG['img_size']), ToTensorV2()])\n",
    "    ds = SegmentationDataset(img_dir, lbl_dir, transform=eda_transform)\n",
    "\n",
    "    if len(ds) == 0:\n",
    "        print(\"âš ï¸ Dataset vide.\")\n",
    "        return\n",
    "\n",
    "    # 1. Distribution des classes (sur l'ensemble du dataset)\n",
    "    print(f\"   Calcul de la distribution des classes sur l'ensemble du dataset ({len(ds)} images)...\")\n",
    "\n",
    "    # RÃ©cupÃ©ration directe des chemins de masques pour Ã©viter de charger les images (plus rapide)\n",
    "    mask_files = sorted(glob.glob(os.path.join(lbl_dir, '*.*')))\n",
    "\n",
    "    total_class_counts = np.zeros(NUM_CLASSES, dtype=np.int64)\n",
    "    total_pixels_overall = 0\n",
    "\n",
    "    for mask_path in tqdm(mask_files, desc=\"Comptage des pixels\"):\n",
    "        # Lecture en niveaux de gris\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None: continue\n",
    "\n",
    "        # Application du mapping (0-33 -> 0-7)\n",
    "        mask_mapped = id_to_group[mask]\n",
    "\n",
    "        unique, counts = np.unique(mask_mapped, return_counts=True)\n",
    "        for u, c in zip(unique, counts):\n",
    "            if u < NUM_CLASSES: # This ensures only mapped classes are counted, 255 (ignore) is excluded\n",
    "                total_class_counts[u] += c\n",
    "\n",
    "        total_pixels_overall += mask.size\n",
    "\n",
    "    # Calcul des pourcentages\n",
    "    class_distribution = {i: (count / total_pixels_overall * 100) if total_pixels_overall > 0 else 0\n",
    "                          for i, count in enumerate(total_class_counts)}\n",
    "\n",
    "    print(\"\\nðŸ“Š Distribution des classes (en % de pixels):\")\n",
    "    # Correct group_names for 8 classes (0-7)\n",
    "    group_names = ['Flat', 'Human', 'Vehicle', 'Construction', 'Object', 'Nature', 'Sky', 'Void']\n",
    "    for i, pct in class_distribution.items():\n",
    "        print(f\"  {group_names[i]}: {pct:.2f}%\")\n",
    "\n",
    "    # CrÃ©ation et sauvegarde du DataFrame de distribution\n",
    "    df_dist = pd.DataFrame({\n",
    "        'Classe': group_names,\n",
    "        'Nombre de Pixels': total_class_counts,\n",
    "        'Pourcentage (%)': [class_distribution[i] for i in range(NUM_CLASSES)]\n",
    "    })\n",
    "    dist_csv_path = os.path.join(CONFIG['output_dir'], 'class_distribution.csv')\n",
    "    df_dist.to_csv(dist_csv_path, index=False)\n",
    "    print(f\"\\nðŸ’¾ Distribution des classes sauvegardÃ©e dans : {dist_csv_path}\")\n",
    "\n",
    "    # Calcul des poids de classes (Balanced: N_total / (N_classes * N_count))\n",
    "    # On ajoute un epsilon pour Ã©viter la division par zÃ©ro\n",
    "    epsilon = 1e-6\n",
    "    class_weights = total_pixels_overall / (NUM_CLASSES * (total_class_counts + epsilon))\n",
    "\n",
    "    # Sauvegarde des poids\n",
    "    weights_path = os.path.join(CONFIG['output_dir'], 'class_weights.json')\n",
    "    with open(weights_path, 'w') as f:\n",
    "        json.dump(class_weights.tolist(), f, indent=4)\n",
    "    print(f\"\\nâš–ï¸ Poids des classes calculÃ©s et sauvegardÃ©s dans : {weights_path}\")\n",
    "    for i, w in enumerate(class_weights):\n",
    "        print(f\"  - {group_names[i]}: {w:.4f}\")\n",
    "\n",
    "    # Affichage Bar Chart\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    # Use all group_colors (which now contains 8 colors)\n",
    "    norm_colors = [np.array(c)/255. for c in group_colors]\n",
    "    bars = plt.bar(range(NUM_CLASSES), total_class_counts, color=norm_colors, edgecolor='black')\n",
    "    plt.title(\"Distribution des Classes (Pixels)\")\n",
    "    plt.xlabel(\"ID Classe (Groupe)\")\n",
    "    plt.ylabel(\"Nombre de Pixels\")\n",
    "    plt.xticks(range(NUM_CLASSES), group_names, rotation=45, ha='right') # Add rotation for long names\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout() # Ensure labels fit\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Visualisation Exemples\n",
    "    print(\"   Exemples d'images et masques...\")\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    sample_indices = np.random.choice(len(ds), 4, replace=False)\n",
    "\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        img, mask = ds[idx]\n",
    "        # Image: Tensor (C,H,W) -> Numpy (H,W,C)\n",
    "        img_np = img.permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Masque: Tensor (H,W) -> Color Map\n",
    "        mask_np = mask.numpy()\n",
    "        color_mask = np.zeros((*mask_np.shape, 3), dtype=np.uint8)\n",
    "        for cid, color in enumerate(group_colors): # This will loop through 8 colors\n",
    "            color_mask[mask_np == cid] = color\n",
    "\n",
    "        axes[0, i].imshow(img_np)\n",
    "        axes[0, i].set_title(f\"Image {idx}\")\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "        axes[1, i].imshow(color_mask)\n",
    "        axes[1, i].set_title(f\"Masque {idx}\")\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "perform_eda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. EntraÃ®nement des ModÃ¨les\n",
    "Cette section contient les fonctions pour entraÃ®ner le modÃ¨le **Mini-Unet** (avec PyTorch standard) et les modÃ¨les **YOLO** (via la librairie Ultralytics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EntraÃ®nement ---\n",
    "def cutmix_data(img1, mask1, img2, mask2, img_size):\n",
    "    \"\"\"ImplÃ©mentation de CutMix pour TensorFlow.\"\"\"\n",
    "    alpha = 1.0\n",
    "    lam = tf.random.gamma(shape=[], alpha=alpha, dtype=tf.float32)\n",
    "    \n",
    "    H, W = img_size[0], img_size[1]\n",
    "    \n",
    "    cut_rat = tf.sqrt(1.0 - lam)\n",
    "    cut_w = tf.cast(tf.cast(W, tf.float32) * cut_rat, tf.int32)\n",
    "    cut_h = tf.cast(tf.cast(H, tf.float32) * cut_rat, tf.int32)\n",
    "    \n",
    "    cut_x = tf.random.uniform([], 0, W, dtype=tf.int32)\n",
    "    cut_y = tf.random.uniform([], 0, H, dtype=tf.int32)\n",
    "    \n",
    "    bbx1 = tf.clip_by_value(cut_x - cut_w // 2, 0, W)\n",
    "    bby1 = tf.clip_by_value(cut_y - cut_h // 2, 0, H)\n",
    "    bbx2 = tf.clip_by_value(cut_x + cut_w // 2, 0, W)\n",
    "    bby2 = tf.clip_by_value(cut_y + cut_h // 2, 0, H)\n",
    "    \n",
    "    # CrÃ©ation du masque binaire pour la zone dÃ©coupÃ©e\n",
    "    indices = tf.meshgrid(tf.range(H), tf.range(W), indexing='ij')\n",
    "    mask_cut = (indices[0] >= bby1) & (indices[0] < bby2) & (indices[1] >= bbx1) & (indices[1] < bbx2)\n",
    "    mask_cut = tf.cast(tf.expand_dims(mask_cut, -1), tf.float32)\n",
    "    \n",
    "    img = img1 * (1 - mask_cut) + img2 * mask_cut\n",
    "    mask = mask1 * (1 - mask_cut) + mask2 * mask_cut\n",
    "    \n",
    "    return img, mask\n",
    "\n",
    "def mixup_data(img1, mask1, img2, mask2):\n",
    "    \"\"\"ImplÃ©mentation de MixUp pour TensorFlow.\"\"\"\n",
    "    alpha = 0.2\n",
    "    lam = tf.random.gamma(shape=[], alpha=alpha, dtype=tf.float32)\n",
    "    lam = tf.reshape(lam, [])\n",
    "    \n",
    "    img = img1 * lam + img2 * (1 - lam)\n",
    "    mask = mask1 * lam + mask2 * (1 - lam)\n",
    "    return img, mask\n",
    "\n",
    "def TFDataGenerator(image_paths, mask_paths, batch_size, img_size, augmentation_type='none', albumentations_transform=None):\n",
    "    \"\"\"CrÃ©e un dataset TensorFlow Ã  partir de listes de chemins avec augmentation configurable.\"\"\"\n",
    "    path_dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
    "\n",
    "    def load_and_preprocess(img_path_tensor, mask_path_tensor):\n",
    "        img = tf.io.read_file(img_path_tensor)\n",
    "        img = tf.image.decode_png(img, channels=3)\n",
    "        img = tf.image.resize(img, img_size, method='bilinear')\n",
    "        img = tf.cast(img, tf.float32) / 255.0\n",
    "\n",
    "        mask = tf.io.read_file(mask_path_tensor)\n",
    "        mask = tf.image.decode_png(mask, channels=1)\n",
    "        mask = tf.image.resize(mask, img_size, method='nearest')\n",
    "        mask = tf.cast(mask, tf.int32)\n",
    "\n",
    "        flat_mask = tf.reshape(mask, [-1])\n",
    "        mapped_mask = tf.gather(tf.constant(id_to_group, dtype=tf.int32), flat_mask)\n",
    "        mask = tf.reshape(mapped_mask, tf.shape(mask))\n",
    "        # Explicitly add channel dimension if it's not already there\n",
    "        if mask.shape.ndims == 2:\n",
    "             mask = tf.expand_dims(mask, axis=-1)\n",
    "        # Ensure mask has a channel dimension of 1\n",
    "        elif mask.shape.ndims == 3 and mask.shape[-1] != 1:\n",
    "             mask = tf.expand_dims(tf.squeeze(mask, axis=-1), axis=-1) # Squeeze and re-add if channel dim exists but is not 1\n",
    "        # Ensure the shape is known\n",
    "        mask = tf.ensure_shape(mask, [img_size[0], img_size[1], 1])\n",
    "        # Cast mask to float32 for augmentation operations\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        return img, mask\n",
    "\n",
    "    dataset = path_dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Mettre en cache le dataset aprÃ¨s le chargement et le redimensionnement pour accÃ©lÃ©rer les Ã©poques suivantes\n",
    "    dataset = dataset.cache()\n",
    "\n",
    "    if augmentation_type in ['cutmix', 'mixup']:\n",
    "        dataset_zipped = tf.data.Dataset.zip((dataset, dataset.shuffle(batch_size * 10)))\n",
    "        if augmentation_type == 'cutmix':\n",
    "             dataset = dataset_zipped.map(lambda ds1, ds2: cutmix_data(ds1[0], ds1[1], ds2[0], ds2[1], img_size), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        elif augmentation_type == 'mixup':\n",
    "             dataset = dataset_zipped.map(lambda ds1, ds2: mixup_data(ds1[0], ds1[1], ds2[0], ds2[1]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Cast the mask back to int32 and squeeze the last dimension after any augmentation steps\n",
    "    dataset = dataset.map(lambda x, y: (x, tf.squeeze(tf.cast(y, tf.int32), axis=-1)), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def get_unet_vgg16_model(input_shape, num_classes):\n",
    "    \"\"\"Construit un modÃ¨le U-Net avec encodeur VGG16 en utilisant Keras.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder (VGG16)\n",
    "    base_model = applications.VGG16(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "    \n",
    "    # Skip connections\n",
    "    s1 = base_model.get_layer(\"block1_conv2\").output\n",
    "    s2 = base_model.get_layer(\"block2_conv2\").output\n",
    "    s3 = base_model.get_layer(\"block3_conv3\").output\n",
    "    s4 = base_model.get_layer(\"block4_conv3\").output\n",
    "    bridge = base_model.get_layer(\"block5_conv3\").output\n",
    "\n",
    "    # Decoder\n",
    "    def decoder_block(input_tensor, skip_tensor, filters):\n",
    "        x = layers.Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding=\"same\")(input_tensor)\n",
    "        x = layers.Concatenate()([x, skip_tensor])\n",
    "        x = layers.Conv2D(filters, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "        x = layers.Conv2D(filters, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "        return x\n",
    "\n",
    "    d1 = decoder_block(bridge, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    outputs = layers.Conv2D(num_classes, (1, 1), activation=\"softmax\")(d4)\n",
    "    \n",
    "    model = models.Model(inputs, outputs, name=\"Mini-Unet-VGG16\")\n",
    "    return model\n",
    "\n",
    "def train_mini_unet():\n",
    "    \"\"\"EntraÃ®ne un modÃ¨le U-Net avec backbone VGG16 (TensorFlow/Keras).\"\"\"\n",
    "    print(f\"\\nðŸš€ EntraÃ®nement Mini-Unet (TF) - {CONFIG['img_size']} ({NUM_CLASSES} Classes)...\")\n",
    "    img_dir, lbl_dir = CONFIG['mini_unet']['img_dir'], CONFIG['mini_unet']['lbl_dir']\n",
    "    if not os.path.exists(img_dir): return None, {}\n",
    "\n",
    "    # RÃ©cupÃ©ration des chemins\n",
    "    image_paths = sorted(glob.glob(os.path.join(img_dir, '*.*')))\n",
    "    mask_paths = sorted(glob.glob(os.path.join(lbl_dir, '*.*')))\n",
    "    \n",
    "    # Split Train/Val (80/20)\n",
    "    split_idx = int(0.8 * len(image_paths))\n",
    "    train_img_paths, val_img_paths = image_paths[:split_idx], image_paths[split_idx:]\n",
    "    train_mask_paths, val_mask_paths = mask_paths[:split_idx], mask_paths[split_idx:]\n",
    "\n",
    "    # Configuration de l'augmentation\n",
    "    aug_type = 'cutmix' if CONFIG['mini_unet']['use_cutmix'] else 'none'\n",
    "\n",
    "    # CrÃ©ation des datasets TF\n",
    "    train_ds = TFDataGenerator(train_img_paths, train_mask_paths, \n",
    "                               CONFIG['mini_unet']['batch_size'], \n",
    "                               CONFIG['img_size'], \n",
    "                               augmentation_type=aug_type)\n",
    "    \n",
    "    val_ds = TFDataGenerator(val_img_paths, val_mask_paths, \n",
    "                             CONFIG['mini_unet']['batch_size'], \n",
    "                             CONFIG['img_size'], \n",
    "                             augmentation_type='none')\n",
    "\n",
    "    # CrÃ©ation du modÃ¨le\n",
    "    model = get_unet_vgg16_model(input_shape=(CONFIG['img_size'][0], CONFIG['img_size'][1], 3), num_classes=NUM_CLASSES)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=CONFIG['mini_unet']['lr']),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # EntraÃ®nement\n",
    "    start = time.time()\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=CONFIG['mini_unet']['epochs'], verbose=1)\n",
    "\n",
    "    # Sauvegarde des courbes d'entraÃ®nement\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss'); plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss'); plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Acc'); plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "    plt.title('Accuracy'); plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG['output_dir'], 'mini_unet_curves.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Ã‰valuation et MÃ©triques\n",
    "    # Calcul manuel du mIoU sur le dataset de validation pour Ãªtre comparable\n",
    "    all_preds, all_trues = [], []\n",
    "    \n",
    "    # Pour l'infÃ©rence, on itÃ¨re sur le dataset de validation\n",
    "    # Note: val_ds est batchÃ©, on dÃ©-batch pour l'Ã©valuation mÃ©trique prÃ©cise ou on prÃ©dit par batch\n",
    "    print(\"Calcul des mÃ©triques finales...\")\n",
    "    for imgs, msks in val_ds:\n",
    "        preds = model.predict(imgs, verbose=0)\n",
    "        pred_masks = tf.argmax(preds, axis=-1).numpy()\n",
    "        true_masks = msks.numpy()\n",
    "        \n",
    "        all_preds.extend(pred_masks.flatten())\n",
    "        all_trues.extend(true_masks.flatten())\n",
    "\n",
    "    inf_t = ((time.time()-start)/len(val_img_paths))*1000 # Temps moyen par image (approximatif global)\n",
    "    \n",
    "    # Sauvegarde de la matrice de confusion\n",
    "    cm = confusion_matrix(all_trues, all_preds, labels=range(NUM_CLASSES))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Flat', 'Human', 'Vehicle', 'Constr.', 'Object', 'Nature', 'Sky', 'Void'])\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    disp.plot(ax=ax, cmap='Blues', xticks_rotation='vertical', values_format='d')\n",
    "    plt.savefig(os.path.join(CONFIG['output_dir'], 'mini_unet_cm.png'))\n",
    "    plt.close()\n",
    "\n",
    "    acc, iou = calculate_pixel_metrics(np.array(all_trues), np.array(all_preds))\n",
    "    return model, {'Model': 'Mini-Unet (TF)', 'mIoU': iou, 'InfÃ©rence (ms)': inf_t}\n",
    "\n",
    "def train_yolo_model(y_cfg):\n",
    "    \"\"\"EntraÃ®ne un modÃ¨le YOLO pour la segmentation.\"\"\"\n",
    "    print(f\"\\nðŸš€ EntraÃ®nement {y_cfg['name']} (Inclut toutes les classes 0-{NUM_CLASSES-1})...\")\n",
    "    # Correction du YAML avant de charger le modÃ¨le\n",
    "    fix_yaml_paths(y_cfg['data'])\n",
    "\n",
    "    model = YOLO(y_cfg['version'])\n",
    "    model.train(\n",
    "        data=y_cfg['data'],\n",
    "        epochs=CONFIG['mini_unet']['epochs'],\n",
    "        imgsz=CONFIG['img_size'][0],\n",
    "        project=CONFIG['yolo_runs_dir'],\n",
    "        name=y_cfg['name'],\n",
    "        # Inclut toutes les classes (0 Ã  NUM_CLASSES-1)\n",
    "        classes=list(range(NUM_CLASSES)), # Changed this line\n",
    "        exist_ok=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    # Validation sur les mÃªmes classes\n",
    "    metrics = model.val(imgsz=CONFIG['img_size'][0], verbose=False)\n",
    "    miou = metrics.seg.map if hasattr(metrics, 'seg') else metrics.box.map\n",
    "    return model, {'Model': y_cfg['name'], 'mIoU': miou, 'InfÃ©rence (ms)': metrics.speed.get('inference', 0.0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisation\n",
    "Fonction utilitaire pour superposer les masques prÃ©dits sur les images originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualisation ---\n",
    "\n",
    "def get_prediction_viz(model, model_name, img_path):\n",
    "    \"\"\"GÃ©nÃ¨re une visualisation de la prÃ©diction (Overlay).\"\"\"\n",
    "    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    if isinstance(model, YOLO):\n",
    "        # InfÃ©rence pour YOLO\n",
    "        results = model.predict(img_path, imgsz=CONFIG['img_size'][0], conf=0.25, verbose=False)\n",
    "        return cv2.resize(cv2.cvtColor(results[0].plot(), cv2.COLOR_BGR2RGB), (w, h))\n",
    "    elif isinstance(model, torch.nn.Module):\n",
    "        # InfÃ©rence pour U-Net\n",
    "        model.eval()\n",
    "        t = A.Compose([A.Resize(*CONFIG['img_size']), A.Normalize(), ToTensorV2()])\n",
    "        inp = t(image=img)['image'].unsqueeze(0).to(CONFIG['device'])\n",
    "        with torch.no_grad():\n",
    "            mask = torch.argmax(model(inp), dim=1).squeeze().cpu().numpy().astype(np.uint8)\n",
    "        mask_r = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "        color_m = np.zeros_like(img)\n",
    "        for i, color in enumerate(group_colors): color_m[mask_r == i] = color\n",
    "        return cv2.addWeighted(img, 0.5, color_m, 0.5, 0)\n",
    "    elif isinstance(model, (tf.keras.Model, tf.Module)):\n",
    "        # InfÃ©rence pour TensorFlow/Keras\n",
    "        img_tensor = tf.io.read_file(img_path)\n",
    "        img_tensor = tf.image.decode_png(img_tensor, channels=3)\n",
    "        img_tensor = tf.image.resize(img_tensor, CONFIG['img_size'])\n",
    "        img_tensor = tf.cast(img_tensor, tf.float32) / 255.0\n",
    "        img_tensor = tf.expand_dims(img_tensor, axis=0)\n",
    "        \n",
    "        pred = model.predict(img_tensor, verbose=0)\n",
    "        mask = tf.argmax(pred, axis=-1)\n",
    "        mask = mask[0].numpy().astype(np.uint8)\n",
    "        \n",
    "        mask_r = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "        color_m = np.zeros_like(img)\n",
    "        for i, color in enumerate(group_colors): color_m[mask_r == i] = color\n",
    "        return cv2.addWeighted(img, 0.5, color_m, 0.5, 0)\n",
    "\n",
    "def generate_pdf_report(df, output_dir, yolo_runs_dir):\n",
    "    \"\"\"GÃ©nÃ¨re un rapport PDF complet avec tableaux et graphiques.\"\"\"\n",
    "    print(\"\\nðŸ“„ GÃ©nÃ©ration du rapport PDF...\")\n",
    "    pdf_path = os.path.join(output_dir, 'report.pdf')\n",
    "    \n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        # Page 1: Tableau des rÃ©sultats\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.axis('tight'); ax.axis('off')\n",
    "        table_data = [df.columns.values.tolist()] + df.values.tolist()\n",
    "        table = ax.table(cellText=table_data, colLabels=None, cellLoc='center', loc='center')\n",
    "        table.auto_set_font_size(False); table.set_fontsize(10)\n",
    "        plt.title(\"Tableau Comparatif des Performances\")\n",
    "        pdf.savefig(fig); plt.close()\n",
    "        \n",
    "        # Pages par modÃ¨le\n",
    "        for _, row in df.iterrows():\n",
    "            model_name = row['Model']\n",
    "            fig = plt.figure(figsize=(11, 8))\n",
    "            plt.axis('off'); plt.title(f\"Rapport: {model_name}\", fontsize=16, fontweight='bold')\n",
    "            \n",
    "            if 'YOLO' in model_name:\n",
    "                run_path = os.path.join(yolo_runs_dir, model_name)\n",
    "                curves_path = os.path.join(run_path, 'results.png')\n",
    "                cm_path = os.path.join(run_path, 'confusion_matrix_normalized.png')\n",
    "                if not os.path.exists(cm_path): cm_path = os.path.join(run_path, 'confusion_matrix.png')\n",
    "            else:\n",
    "                curves_path = os.path.join(output_dir, 'mini_unet_curves.png')\n",
    "                cm_path = os.path.join(output_dir, 'mini_unet_cm.png')\n",
    "                \n",
    "            if curves_path and os.path.exists(curves_path):\n",
    "                ax1 = fig.add_axes([0.1, 0.5, 0.8, 0.4]); ax1.imshow(plt.imread(curves_path)); ax1.axis('off'); ax1.set_title(\"Courbes d'entraÃ®nement\")\n",
    "            if cm_path and os.path.exists(cm_path):\n",
    "                ax2 = fig.add_axes([0.1, 0.05, 0.8, 0.4]); ax2.imshow(plt.imread(cm_path)); ax2.axis('off'); ax2.set_title(\"Matrice de Confusion\")\n",
    "            pdf.savefig(fig); plt.close()\n",
    "    print(f\"âœ… Rapport PDF sauvegardÃ© : {pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ExÃ©cution Principale et Comparaison\n",
    "Lancement du pipeline complet : entraÃ®nement de tous les modÃ¨les, comparaison des performances et affichage d'une galerie de rÃ©sultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list, trained_models = [], {}\n",
    "\n",
    "# 1. EntraÃ®nement Mini-Unet\n",
    "m_unet, met_u = train_mini_unet()\n",
    "if m_unet:\n",
    "    trained_models['Mini-Unet'] = m_unet\n",
    "    results_list.append(met_u)\n",
    "\n",
    "# 2. EntraÃ®nement ModÃ¨les YOLO (V8, V9, V11)\n",
    "for yolo_cfg in CONFIG['yolo_models']:\n",
    "    try:\n",
    "        m_yolo, met_y = train_yolo_model(yolo_cfg)\n",
    "        trained_models[yolo_cfg['name']] = m_yolo\n",
    "        results_list.append(met_y)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Erreur {yolo_cfg['name']}: {e}\")\n",
    "\n",
    "if results_list:\n",
    "    # CrÃ©ation du DataFrame de rÃ©sultats\n",
    "    df = pd.DataFrame(results_list)\n",
    "    print(\"\\nðŸ“Š RÃ‰SULTATS FINAUX (256x256 + CutMix) :\")\n",
    "    print(df.to_string(index=False))\n",
    "    df.to_csv(os.path.join(CONFIG['output_dir'], 'comparison_results.csv'), index=False)\n",
    "\n",
    "    # 3. Rapport et Export\n",
    "    print(\"\\nðŸ“Š TABLEAU RÃ‰CAPITULATIF DES PERFORMANCES (Taille: 256x256) :\")\n",
    "    print(\"=\" * 60)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"=\" * 60)\n",
    "    csv_path = os.path.join(CONFIG['output_dir'], 'final_model_comparison_256.csv')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    # GÃ©nÃ©ration du rapport PDF\n",
    "    generate_pdf_report(df, CONFIG['output_dir'], CONFIG['yolo_runs_dir'])\n",
    "\n",
    "# 4. Galerie de Comparaison\n",
    "test_files = glob.glob(os.path.join(CONFIG['test_images_dir'], '*.*'))\n",
    "if test_files and trained_models:\n",
    "    print(f\"\\nðŸŽ¨ GÃ©nÃ©ration des galeries de comparaison...\")\n",
    "    for img_p in test_files[:3]:\n",
    "        fname = os.path.basename(img_p)\n",
    "        n_models = len(trained_models)\n",
    "        fig, axes = plt.subplots(1, n_models + 1, figsize=(5 * (n_models + 1), 5))\n",
    "\n",
    "        # Image Originale\n",
    "        axes[0].imshow(cv2.cvtColor(cv2.imread(img_p), cv2.COLOR_BGR2RGB))\n",
    "        axes[0].set_title(\"Original\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        # PrÃ©dictions des modÃ¨les\n",
    "        for i, (name, m_obj) in enumerate(trained_models.items()):\n",
    "            axes[i+1].imshow(get_prediction_viz(m_obj, name, img_p))\n",
    "            axes[i+1].set_title(name)\n",
    "            axes[i+1].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(CONFIG['output_dir'], f\"comparison_256_{fname}\"))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Optimisation du Meilleur ModÃ¨le YOLO\n",
    "print(\"\\nðŸ”§ Recherche et Optimisation du meilleur modÃ¨le YOLO...\")\n",
    "yolo_results = df[df['Model'].str.contains('YOLO')]\n",
    "if not yolo_results.empty:\n",
    "    best_yolo_idx = yolo_results['mIoU'].idxmax()\n",
    "    best_yolo_name = yolo_results.loc[best_yolo_idx, 'Model']\n",
    "    print(f\"ðŸ† Meilleur modÃ¨le YOLO identifiÃ© : {best_yolo_name}\")\n",
    "\n",
    "    if best_yolo_name in trained_models:\n",
    "        model_to_opt = trained_models[best_yolo_name]\n",
    "        yolo_cfg = next((cfg for cfg in CONFIG['yolo_models'] if cfg['name'] == best_yolo_name), None)\n",
    "\n",
    "    # A. Export pour infÃ©rence optimisÃ©e (ONNX)\n",
    "    print(\"ðŸ“¦ Export vers ONNX pour accÃ©lÃ©ration de l'infÃ©rence...\")\n",
    "    try:\n",
    "        model_to_opt.export(format='onnx', imgsz=CONFIG['img_size'][0])\n",
    "        print(\"âœ… Export ONNX terminÃ© avec succÃ¨s.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Erreur Export: {e}\")\n",
    "\n",
    "    # B. Optimisation des HyperparamÃ¨tres (Exemple rapide)\n",
    "    print(\"ðŸ§¬ Lancement d'un tuning rapide des hyperparamÃ¨tres (5 epochs, 5 itÃ©rations)...\")\n",
    "    try:\n",
    "        if yolo_cfg:\n",
    "            model_to_opt.tune(data=yolo_cfg['data'], epochs=5, iterations=5, imgsz=CONFIG['img_size'][0], plots=False, verbose=False)\n",
    "            print(\"âœ… Tuning terminÃ©.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Erreur Tuning: {e}\")\n",
    "\n",
    "    # C. Sauvegarde du modÃ¨le final\n",
    "    print(f\"ðŸ’¾ Sauvegarde du modÃ¨le final {best_yolo_name}...\")\n",
    "    try:\n",
    "        # Copie du meilleur checkpoint de l'entraÃ®nement\n",
    "        source_path = os.path.join(CONFIG['yolo_runs_dir'], best_yolo_name, 'weights', 'best.pt')\n",
    "        dest_path = os.path.join(CONFIG['output_dir'], f'final_best_{best_yolo_name}.pt')\n",
    "        if os.path.exists(source_path):\n",
    "            import shutil\n",
    "            shutil.copy(source_path, dest_path)\n",
    "            print(f\"âœ… ModÃ¨le final copiÃ© vers : {dest_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Erreur Sauvegarde: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… Pipeline terminÃ© avec succÃ¨s (256x256 + CutMix).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Chargement, InfÃ©rence et Analyse Approfondie\n",
    "Dans cette section, nous chargeons le modÃ¨le YOLOv9 sauvegardÃ© pour effectuer une infÃ©rence sur une image inÃ©dite. Nous affichons Ã©galement la matrice de confusion gÃ©nÃ©rÃ©e lors de la validation pour analyser les erreurs de classification, ainsi que le tableau rÃ©capitulatif des mÃ©triques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Chargement du modÃ¨le final, InfÃ©rence et Matrice de Confusion\n",
    "print(\"\\nðŸ” Chargement du modÃ¨le final et analyse...\")\n",
    "\n",
    "# Use the variable if defined, otherwise try to find it\n",
    "if 'best_yolo_name' in locals():\n",
    "    final_model_path = os.path.join(CONFIG['output_dir'], f'final_best_{best_yolo_name}.pt')\n",
    "else:\n",
    "    # Fallback search\n",
    "    potential_files = glob.glob(os.path.join(CONFIG['output_dir'], 'final_best_*.pt'))\n",
    "    final_model_path = potential_files[0] if potential_files else \"\"\n",
    "\n",
    "if os.path.exists(final_model_path):\n",
    "    # A. Chargement\n",
    "    print(f\"ðŸ“¥ Chargement de {final_model_path}...\")\n",
    "    loaded_model = YOLO(final_model_path)\n",
    "\n",
    "    # B. InfÃ©rence sur des images de test (Original vs PrÃ©diction)\n",
    "    test_files = glob.glob(os.path.join(CONFIG['test_images_dir'], '*.*'))\n",
    "    if test_files:\n",
    "        print(f\"ðŸ–¼ï¸ Visualisation des rÃ©sultats sur {min(3, len(test_files))} images de test...\")\n",
    "        for img_path in test_files[:3]:\n",
    "            # Image originale\n",
    "            img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # PrÃ©diction\n",
    "            results = loaded_model.predict(img_path, imgsz=CONFIG['img_size'][0], conf=0.25, verbose=False)\n",
    "            res_plotted = results[0].plot()\n",
    "            res_plotted = cv2.cvtColor(res_plotted, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Affichage cÃ´te Ã  cÃ´te\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title(f\"Original : {os.path.basename(img_path)}\")\n",
    "            axes[0].axis('off')\n",
    "\n",
    "            axes[1].imshow(res_plotted)\n",
    "            axes[1].set_title(f\"PrÃ©diction YOLOv9\")\n",
    "            axes[1].axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    # C. Matrice de Confusion et MÃ©triques\n",
    "    print(\"ðŸ“‰ Ã‰valuation du modÃ¨le optimisÃ© sur le jeu de validation...\")\n",
    "    # Reuse yolo_cfg from optimization block if available, or find it based on filename\n",
    "    if 'yolo_cfg' in locals() and yolo_cfg:\n",
    "        # Validation pour gÃ©nÃ©rer la matrice (plots=True est essentiel)\n",
    "        metrics = loaded_model.val(data=yolo_cfg['data'], imgsz=CONFIG['img_size'][0], plots=True, verbose=False)\n",
    "\n",
    "        print(f\"\\nðŸ“Š MÃ©triques aprÃ¨s optimisation ({best_yolo_name}):\")\n",
    "        print(f\"   mAP50-95: {metrics.seg.map:.4f}\")\n",
    "        print(f\"   mAP50:    {metrics.seg.map50:.4f}\")\n",
    "\n",
    "        # Recherche de l'image de la matrice de confusion\n",
    "        cm_path = os.path.join(metrics.save_dir, 'confusion_matrix_normalized.png')\n",
    "        if not os.path.exists(cm_path):\n",
    "             cm_path = os.path.join(metrics.save_dir, 'confusion_matrix.png')\n",
    "\n",
    "        if os.path.exists(cm_path):\n",
    "            print(f\"âœ… Affichage de la matrice de confusion : {cm_path}\")\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            plt.imshow(cv2.cvtColor(cv2.imread(cm_path), cv2.COLOR_BGR2RGB))\n",
    "            plt.title(\"Matrice de Confusion NormalisÃ©e (YOLOv9)\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            # Sauvegarde sÃ©parÃ©e de la matrice de confusion optimisÃ©e\n",
    "            import shutil\n",
    "            shutil.copy(cm_path, os.path.join(CONFIG['output_dir'], 'optimized_confusion_matrix.png'))\n",
    "        else:\n",
    "            print(\"âš ï¸ Impossible de trouver l'image de la matrice de confusion.\")\n",
    "            \n",
    "    # D. Sauvegarde des rÃ©sultats optimisÃ©s sÃ©parÃ©s\n",
    "    print(\"\\nðŸ’¾ Sauvegarde des outputs optimisÃ©s sÃ©parÃ©s...\")\n",
    "    \n",
    "    # 1. Tableau de mÃ©triques optimisÃ©\n",
    "    opt_metrics_data = [{\n",
    "        'Model': f'{best_yolo_name}_Optimized',\n",
    "        'mIoU': metrics.seg.map,\n",
    "        'mAP50': metrics.seg.map50,\n",
    "        'Precision': metrics.seg.mp,\n",
    "        'Recall': metrics.seg.mr\n",
    "    }]\n",
    "    opt_csv_path = os.path.join(CONFIG['output_dir'], 'optimized_model_results.csv')\n",
    "    pd.DataFrame(opt_metrics_data).to_csv(opt_csv_path, index=False)\n",
    "    print(f\"   - Tableau de mÃ©triques : {opt_csv_path}\")\n",
    "    \n",
    "    # 2. Images de prÃ©diction optimisÃ©es\n",
    "    if test_files:\n",
    "        for img_path in test_files[:3]:\n",
    "            results = loaded_model.predict(img_path, imgsz=CONFIG['img_size'][0], conf=0.25, verbose=False)\n",
    "            res_plotted = results[0].plot()\n",
    "            fname = os.path.basename(img_path)\n",
    "            save_path = os.path.join(CONFIG['output_dir'], f\"optimized_prediction_{fname}\")\n",
    "            cv2.imwrite(save_path, res_plotted)\n",
    "            print(f\"   - Image optimisÃ©e : {save_path}\")\n",
    "\n",
    "# D. Affichage du Tableau RÃ©capitulatif (Comparaison Initiale)\n",
    "csv_path = os.path.join(CONFIG['output_dir'], 'final_model_comparison_256.csv')\n",
    "if os.path.exists(csv_path):\n",
    "    print(\"\\nðŸ“Š Rappel des Performances (Comparaison Initiale) :\")\n",
    "    df_results = pd.read_csv(csv_path)\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(df_results)\n",
    "    except ImportError:\n",
    "        print(df_results.to_string(index=False))\n",
    "else:\n",
    "    print(\"âš ï¸ Pas de fichier de rÃ©sultats trouvÃ©.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "Ce projet a permis de comparer une approche de segmentation sÃ©mantique classique (U-Net) avec des modÃ¨les YOLO rÃ©cents (v8, v9, v11) sur le dataset Cityscapes rÃ©duit.\n",
    "\n",
    "**Points clÃ©s :**\n",
    "- **Performance :** Les modÃ¨les YOLO, en particulier YOLOv9, offrent gÃ©nÃ©ralement un excellent compromis entre prÃ©cision (mIoU) et vitesse d'infÃ©rence.\n",
    "- **Robustesse :** L'utilisation de **CutMix** a permis d'amÃ©liorer la robustesse du modÃ¨le face aux occlusions partielles.\n",
    "- **Optimisation :** L'export ONNX et le tuning des hyperparamÃ¨tres montrent des pistes pour le dÃ©ploiement en temps rÃ©el sur vÃ©hicule autonome.\n",
    "\n",
    "**Perspectives :**\n",
    "- EntraÃ®ner sur l'ensemble complet de Cityscapes.\n",
    "- Tester des techniques d'augmentation plus poussÃ©es (Mosaic, MixUp).\n",
    "- IntÃ©grer un post-traitement (CRF) pour affiner les masques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}